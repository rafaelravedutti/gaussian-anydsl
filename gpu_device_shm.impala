/* Image structure */
struct image {
  buffer : Buffer,          /* Buffer that contains host data */
  device_data : Buffer,     /* Data in the target device */
  width : i32,              /* Image width */
  height : i32              /* Image height */
}

/* Filter structure */
struct filter {
  buffer : Buffer,          /* Buffer that contains target device data */
  width : i32,              /* Filter width */
  height : i32              /* Filter height */
}

/* Shared memory data */
struct shared_memory {
  data : &mut[3][f64],      /* Shared memory data */
  width : i32,              /* Shared memory width */
  offset_x : i32,           /* Offset for x axis */
  offset_y : i32            /* Offset for y axis */
}

/* Iterate from min to max executing a function */
fn range(mut min : i32, max : i32, body : fn(i32) -> ()) -> () {
  /* While maximum value not reached */
  while min < max {
    /* Call the given function */
    body(min);
    /* Go to next iteration */
    min++;
  }
}

/* Return kernel block configuration for device */
fn get_block_config() -> (i32, i32, i32) {
  (128, 1, 1)
}

/* Return kernel grid configuration for device based on the image dimensions and block configuration */
fn get_grid_config(input_image : image) -> (i32, i32, i32) {
  let block_config = get_block_config();
  let gidx_size = input_image.width + (block_config(0) - input_image.width % block_config(0));
  let gidy_size = input_image.height + (block_config(1) - input_image.height % block_config(1));

  (gidx_size, gidy_size, 1)
}

/* Iterate from min to max with a offset executing a function */
fn offset_range(mut min : i32, max : i32, offset : i32, body : fn(i32) -> ()) -> () {
  /* While maximum value not reached */
  while min < max {
    /* Call the given function */
    body(min);
    /* Go to next iteration */
    min += offset;
  }
}

/* Exponential function */
fn exp(a : f64) -> f64 {
  /* Call GPU intrinsic exponential function provided by the AnyDSL runtime library */
  cuda_intrinsics.exp(a)
}

/* Write f64 (double) data to a buffer, considering it as an array of doubles */
fn write_f64(buffer : Buffer, index : i32, value : f64) -> () {
  bitcast[&mut[1][f64]](buffer.data)(index) = value
}

/* Read f64 (double) from a buffer, considering it as an array of doubles */
fn read_f64(buffer : Buffer, index : i32) -> f64 {
  bitcast[&[1][f64]](buffer.data)(index)
}

/* Sets image pixel at (x, y) coordinates */
fn set_pixel(img : image, x : i32, y : i32, value : f64) -> () {
  bitcast[&mut[1][f64]](img.device_data.data)(y * img.width + x) = value
}

/* Gets image pixel at (x, y) coordinates */
fn get_pixel(shared : shared_memory, x : i32, y : i32) -> f64 {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  shared.data((y + shared.offset_y) * shared.width + x + shared.offset_x)
}

/* Gets 1D filter coefficient at i index */
fn get_1d_filter_coeff(filt : filter, i : i32) -> f64 {
  bitcast[&[1][f64]](filt.buffer.data)(i)
}

/* Gets 2D filter coefficient at (i, j) index */
fn get_2d_filter_coeff(filt : filter, i : i32, j : i32) -> f64 {
  bitcast[&[1][f64]](filt.buffer.data)(j * filt.width + i)
}

/* Create temporary image */
fn create_temporary_image(width : i32, height : i32) -> image {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Null buffer that musn't be used */
  let null_buffer = Buffer {
    device: 0,
    data: 0 as &[i8]
  };

  /* Create image object and return it */
  image {
    buffer: null_buffer,
    device_data: accelerator.alloc(width * height * sizeof[f64]()),
    width: width,
    height: height
  }
}

/* Get device data for image */
fn get_image_device_data(img : image) -> Buffer {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* GPU allocation for the image */
  accelerator.alloc(img.width * img.height * sizeof[f64]())
}

/* Release image data from target device */
fn release_image(img : image) -> () {
  release(img.device_data);
}

/* Release filter data from target device */
fn release_filter(filt : filter) -> () {
  release(filt.buffer);
}

/* Specify input and output images where operations will be performed */
fn images(input_image : image, output_image : image, body : fn() -> ()) -> () {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Input image size in bytes */
  let image_size = input_image.width * input_image.height * sizeof[f64]();

  /* Copy input image content to GPU */
  copy(input_image.buffer, input_image.device_data, image_size);

  /* Execute the given operations */
  body();

  /* Copy output image GPU data to output host data */
  copy(output_image.device_data, output_image.buffer, image_size);
}

/* Iterates over an image in GPU */
fn iterate(input_image : image, body : fn(i32, i32) -> ()) -> () {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* CUDA kernel grid configuration */
  let grid = get_grid_config(input_image);
  /* CUDA kernel block configuration */
  let block = get_block_config();

  /* Execute the following code in the GPU */
  with accelerator.exec(grid, block) @{
    /* Image x coordinate in the grid */
    let gid_x = accelerator.bidx() * accelerator.bdimx() + accelerator.tidx();
    /* Image y coordinate in the grid */
    let gid_y = accelerator.bidy() * accelerator.bdimy() + accelerator.tidy();

    /* Check for image boundaries to avoid invalid memory region access */
    if gid_x < input_image.width && gid_y < input_image.height {
      /* Call the body function with the obtained indexes */
      body(gid_x, gid_y);
    }
  }
}

/* Iterates over an image in GPU based on a stencil */
fn iterate_stencil(input_image : image, stencil : filter, body : fn(i32, i32, shared_memory) -> ()) -> () {
  /* Use first CUDA accelerator */
  let acc = cuda_accelerator(0);
  /* CUDA kernel grid configuration */
  let grid = get_grid_config(input_image);
  /* CUDA kernel block configuration */
  let block = get_block_config();

  /* Execute the following code in the GPU */
  with acc.exec(grid, block) @{
    /* Block dimensions */
    let bdimx = acc.bdimx();
    let bdimy = acc.bdimy();
    /* Block indexes */
    let bidx = acc.bidx();
    let bidy = acc.bidy();
    /* Thread indexes */
    let tidx = acc.tidx();
    let tidy = acc.tidy();

    /* Image x coordinate in the grid */
    let gid_x = bidx * bdimx + tidx;
    /* Image y coordinate in the grid */
    let gid_y = bidy * bdimy + tidy;
    /* Width size in pixels to extended the block data in the shared memory */
    let extend_width = stencil.width / 2;
    /* Height size in pixels to extended the block data in the shared memory */
    let extend_height = stencil.height / 2;
    /* Shared memory x dimension size */
    let shm_dimx = bdimx + extend_width * 2;
    /* Shared memory y dimension size */
    let shm_dimy = bdimy + extend_height * 2;

    /* Allocate shared memory for current block with size = (128 + 3 * 2) * (1 + 3 * 2) */
    let shared = shared_memory {
      data: reserve_shared[f64](938),
      width: shm_dimx,
      offset_x: extend_width - bidx * bdimx,
      offset_y: extend_height - bidy * bdimy
    };

    /* Go through each block step in the y axis (rows) */
    for j in offset_range(0, shm_dimy, bdimy) {
      /* Go through each block step in the x axis (columns) */
      for i in offset_range(0, shm_dimx, bdimx) {
        /* Shared memory x axis */
        let shm_index_x = tidx + i;
        /* Shared memory y axis */
        let shm_index_y = tidy + j;

        /* Check shared memory boundaries to avoid invalid memory region access */
        if shm_index_x < shm_dimx && shm_index_y < shm_dimy {
          /* Image x axis */
          let img_index_x = gid_x - extend_width + i;
          /* Image y axis */
          let img_index_y = gid_y - extend_height + j;

          /* Check image boundaries to avoid invalid memory region access */
          if img_index_x >= 0 && img_index_x < input_image.width && img_index_y >= 0 && img_index_y < input_image.height {
            /* Copy input image content to shared memory */
            shared.data(shm_index_y * shm_dimx + shm_index_x) =
              bitcast[&[1][f64]](input_image.device_data.data)(img_index_y * input_image.width + img_index_x);
          }
        }
      }
    }

    /* Thread barrier synchronization */
    acc.barrier();

    /* Check for image boundaries to avoid invalid memory region access */
    if gid_x < input_image.width && gid_y < input_image.height {
      /* Call the body function with the obtained indexes */
      body(gid_x, gid_y, shared);
    }
  }
}

/* Generates a 5x1 filter data structure given its coefficients */
fn filter_5x1(mask : Buffer, is_row : bool) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Filter result */
  let mut result : filter;

  /* Allocates the filter data structure for a 5x1 column filter */
  if !is_row {
    result = filter {
      buffer: accelerator.alloc(5 * sizeof[f64]()),
      width: 5,
      height: 1
    };
  /* Allocates the filter data structure for a 5x1 row filter */
  } else {
    result = filter {
      buffer: accelerator.alloc(5 * sizeof[f64]()),
      width: 1,
      height: 5
    };
  }

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}

/* Generates a 5x5 filter data structure given its coefficients */
fn filter_5x5(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 5x5 filter */
  let result = filter {
    buffer: accelerator.alloc(5 * 5 * sizeof[f64]()),
    width: 5,
    height: 5
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}

/* Generates a 7x1 filter data structure given its coefficients */
fn filter_7x1(mask : Buffer, is_row : bool) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);
  /* Filter result */
  let mut result : filter;

  /* Allocates the filter data structure for a 7x1 column filter */
  if !is_row {
    result = filter {
      buffer: accelerator.alloc(7 * sizeof[f64]()),
      width: 7,
      height: 1
    };
  /* Allocates the filter data structure for a 7x1 row filter */
  } else {
    result = filter {
      buffer: accelerator.alloc(7 * sizeof[f64]()),
      width: 1,
      height: 7
    };
  }

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}

/* Generates a 7x7 filter data structure given its coefficients */
fn filter_7x7(mask : Buffer) -> filter {
  /* Use first CUDA accelerator */
  let accelerator = cuda_accelerator(0);

  /* Allocates the filter data structure for a 7x7 filter */
  let result = filter {
    buffer: accelerator.alloc(7 * 7 * sizeof[f64]()),
    width: 7,
    height: 7
  };

  /* Copy filter data to GPU */
  copy(mask, result.buffer, result.width * result.height * sizeof[f64]());

  result
}
